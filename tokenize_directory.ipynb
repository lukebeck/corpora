{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Set the tokenizer mode as B which might stand for a specific mode according to your tokenizer settings\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "    # Step 1: Split input text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Step 2: Remove any lines that are empty strings\n",
    "    # The list comprehension iterates over all lines and keeps only the ones that are non-empty\n",
    "    non_empty_lines = [line for line in lines if line]\n",
    "\n",
    "    # Step 3: Remove leading and trailing whitespace from each line\n",
    "    # The list comprehension iterates over all non-empty lines and strips the whitespace\n",
    "    trimmed_lines = [line.strip() for line in non_empty_lines]\n",
    "\n",
    "    # Initialize an empty list to store the tokenized lines\n",
    "    tokens = []\n",
    "\n",
    "    # Step 4: Tokenize each line\n",
    "    # For each line, we tokenize it into spans (words or phrases),\n",
    "    # replace spaces with '_', and append the tokenized line to 'tokens'\n",
    "    for line in trimmed_lines:\n",
    "        tokenized_line = [\"_\".join([m.surface(), m.part_of_speech()[0], m.dictionary_form()])\n",
    "                          for m in tokenizer_obj.tokenize(line, mode)]\n",
    "        tokens.append(\" \".join(tokenized_line))\n",
    "\n",
    "    # Step 5: Join the tokenized lines back together with line breaks between them\n",
    "    tokenized_text = \"\\n\".join(tokens)\n",
    "\n",
    "    return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/060.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/074.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/048.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/049.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/075.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/061.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/088.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/077.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/063.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/062.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/076.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/089.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/072.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/066.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/067.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/073.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/059.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/065.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/071.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/070.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/064.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/058.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/003.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/017.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/016.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/002.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/014.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/028.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/029.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/001.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/015.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/039.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/011.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/005.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/004.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/010.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/038.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/006.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/012.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/013.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/007.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/022.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/036.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/037.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/023.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/035.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/021.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/009.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/008.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/020.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/034.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/018.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/030.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/024.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/025.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/031.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/019.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/027.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/033.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/032.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/026.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/082.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/041.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/055.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/069.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/068.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/054.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/040.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/083.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/081.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/056.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/042.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/043.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/057.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/080.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/090.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/084.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/053.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/047.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/046.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/052.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/085.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/091.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/087.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/093.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/078.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/044.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/050.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/051.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/045.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/079.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/092.txt\n",
      "Tokenising /Users/luke/Projects/work/textbook-corpus/utils/nonfiction/086.txt\n",
      "Tokenization process completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory that contains the text files\n",
    "input_dir = '/Users/luke/Projects/work/textbook-corpus/utils/nonfiction/'\n",
    "\n",
    "# Define the directory to save the tokenized files\n",
    "output_dir = os.path.join(input_dir, 'tagged')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Get a list of all .txt files in the input directory\n",
    "txt_files = glob.glob(os.path.join(input_dir, '*.txt'))\n",
    "\n",
    "# Iterate over each text file\n",
    "for txt_file in txt_files:\n",
    "    # Open the text file and read its content\n",
    "    with open(txt_file, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    print(f'Tokenising {txt_file}')\n",
    "    # Tokenize the content of the file\n",
    "    tokenized_content = tokenize(content)\n",
    "\n",
    "    # Create the output file path\n",
    "    # os.path.splitext(txt_file)[0] gets the file name without the extension\n",
    "    # os.path.basename gets the base name of the file (without directories)\n",
    "    output_file = os.path.join(output_dir, os.path.basename(os.path.splitext(txt_file)[0]) + '_tagged.txt')\n",
    "\n",
    "    # Write the tokenized content to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(tokenized_content)\n",
    "\n",
    "print(\"Tokenization process completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
